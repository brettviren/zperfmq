#+title: zperfmq -- a ZeroMQ performance tester


This package provides ways to measure ZeroMQ message throughput, latency, CPU and memory usage.

* Getting started

Get the source:

#+begin_example
  $ git clone https://github.com/brettviren/zperfmq.git
#+end_example

Build via ~autoconf~:

#+begin_example
  $ cd zperfmq
  $ ./configure && make install
#+end_example

Build via CMake:

#+begin_example
  $ cmake zperfmq && make install
#+end_example

Command line interface to a single ~zperf~ /measurement/:

#+begin_example
  $ zperfcli --help
#+end_example

Python bindings:
#+begin_example
  $ python3 -m venv venv && source venv/bin/activate
  $ pip install .
  $ python -c 'from zperfmq import Zperf; Zperf.test(0)'
#+end_example

For that last one to work, the ~libzperfmq.so~ must be found.  A ~zperf~ instance can be created as, eg ~z = Zperf(4)~.

* Overview

The ~libzmq~ package itself provides some very useful performance tools.  The ZeroMQ wiki has guidance on [[http://wiki.zeromq.org/results:perf-howto][using them]] and some [[http://wiki.zeromq.org/area:results][results]] (see also the [[http://wiki.zeromq.org/whitepapers:measuring-performance][performance whitepaper]]).  The ~zperfmq~ package provides much of the same functionality with these additional goals to provide:

- [X] measure CPU% during a performance measurement
- [X] performance testing code as a [[https://github.com/zeromq/czmq/][CZMQ]] actor (~perf_actor~).
- [X] a [[https://github.com/zeromq/zproject/][zproject]] API (~zperf~) to that actor
- [X] a single CLI for performing any measurement
- [ ] a zproject server interface to the ~perf~ actor
- [ ] functionality for ongoing network monitoring
- [ ] hooks to time-series databases (eg fill Graphite via STREAM socket)
- [X] Python bindings
- [ ] automation of measurement campaigns
- [ ] generation of plots of results (with Python/matplotlib)
- [ ] support for PUB/SUB throughput and message loss 

* The ~perf~ actor

The core of this package is the ~perf~ actor.  It is created with a /measurement socket/ of a particular ZMQ type and then may later be given ~BIND~ or ~CONNECT~ command protocol messages in order to bind or connect to a given address.  When a ~BIND~ is requested the address may contain a wildcard for the port (if ~tcp://~) and a fully-qualified address will be returned.

Subsequently, the owner of the ~perf~ actor may initiate various /measurements/ with other the following command protocol messages:

- ~ECHO~ :: receive messages on the /measurement socket/ and immediately send them back until the fixed number of messages are returned.  Return the elapsed time between receipt of first message and sending of final.

- ~YODEL~ :: send and receive a fixed number of messages, return the elapsed time.

- ~SEND~ :: send a fixed number of messages, return elapsed time.

- ~RECV~ :: receive a fixed number of messages, return elapsed time.

As may be clear from the description, ~ECHO~ and ~YODEL~ form a pair as do ~SEND~ and ~RECV~.

~ECHO~ and ~YODEL~ may be used to measure the *round-trip latency* from the point of view of both ends.  The ~ECHO~ end measures the elapsed time starting at the receipt of the first message and so is not sensitive to any delays related to initializing the endpoints.  The elapsed time is, however, sensitive to output buffering so will be substantially quick for measurements using a number of messages comparable to the HWM.

The ~SEND~ and ~RECV~ pair may be used to measure *one-way throughput* from the point of view of either end.  Like ~ECHO~, the ~SEND~ end is also sensitive to output buffering.

A message size is given for ~YODEL~ and ~SEND~ (the message producers).  The ~ECHO~ and ~RECV~ will tally the amount of data they receive and make it available in their measurement responses.

In addition to the elapsed time ("wall clock") the amount of CPU time for the measurement is also tallied.  This is the sum of "user" and "system" time.  The ratio with elapsed time (times 100) will give %CPU used over the measurement.  An example of this using the ~zperf~ API is given below.

Finally, the message producers construct messages where the first frame holds an integer message count starting from zero for each measurement run.  The message consumers (~YODEL~ and ~RECV~) check this count against expectation of a synchronous stream.  Each message that is out of sync increments an ~noos~ counter which is available after a measurement.  Out-of-sync message may occur if multiple I/O threads (eg as set by the ~ZSYS_IO_THREADS~ environment variable) are requested and multiple ~CONNECT~ command messages are sent to a ~perf~ actor for the same address.  Such a configuration may required for reaching throughput above 25 Gbps (on suitable networks).

* The ~zperf~ API

This package provides a ~zproject~ CLASS API called ~zperf~ which provides synchronous methods to the ~perf~ actor command protocol.  Each of the above commands have corresponding ~zperf~ methods.  In the case of each of the four measurement commands there are a trio of methods.

- ~M_ini()~ :: initialize a measurement, typically returns immediately
- ~M_fin()~ :: wait for measurement to finish and return resulting operation time 
- ~M()~ :: call both of the above

Where ~M~ is ~ECHO~, ~YODEL~, ~SEND~, ~RECV~.  The differentiation between ~M_ini()~ and ~M_fin()~ allow for tests driven by different ~zperf~ instances to be interleaved.  This interleaving is required to avoid the deadlock that would occur if a full test is attempted with supplying an active other end.  A test needs independent actors on each end and an attempt to use the same actor for both will still result in deadlock or other errors.

An example synchronous test, ignoring error checking and cleanup:

#+begin_src c
  zperf_t* zpe = zperf_new(ZMQ_REP);
  zperf_t* zpy = zperf_new(ZMQ_REQ);

  const char* ep = zperf_bind(zpy, "tcp://127.0.0.1:*");
  zperf_connect(zpe, ep);

  zperf_echo_ini(zpe, nmsgs);
  int64_t time_y = zperf_yodel(zpy, nmsgs, msgsize);
  int64_t time_e = zperf_echo_fin(zpe);

  uint64_t cpu_y = zperf_cpu(zpy);
  uint64_t cpu_e = zperf_cpu(zpe);

  double cpupc_y = (100.0*cpu_y)/time_y;
  double cpupc_e = (100.0*cpu_e)/time_e;
#+end_src

* Sockets

Currently the following /measurement sockets/ are supported:

- ~ECHO~ :: REP or ROUTEr
- ~YODEL~ :: REQ
- ~SEND~ :: PUSH
- ~RECV~ :: PULL

* Possible Future Extensions

Some ideas for future extension:

- A zproject server which launches a ~perf~ actor on behalf of a client, returning the result.

- A zproject client interface to the above.

- Associate Zyre presence with a ~perf~ actor in the server in order to create networks based on abstract identifiers.

- Support different measurement halting conditions to be robust against message loss (eg when PUB/SUB support is added) and against variable number of messages such as when PUSH/PULL or PUB/SUB networks of greater than 2 nodes are launched.


* Coordination

A pair of ~perf~ actors to not have any internal coordination beyond waiting for the receipt of the first message before starting any stopwatches.  For a single measurement, the only external coordination required is that a sender must stay alive long enough after all messages are sent to allow for any output queues to drain.

A ~perf~ actor can be directed to perform a sequence of measurements and these do not have any additional coordination requirements if the actor employs a single ZeroMQ I/O thread.  However, this changes in the case of using multiple I/O threads and multiple ~connect()~ to a socket.  Such a configuration is needed to achieve better than 20-25 Gbps.   It also results in messages being sent and received out-of-order.  In a sequence of coordinated messages, this out-of-order poses no problem.  If not, initial messages from a subsequent measurement will begin flowing and mix with the last few messages from the previous measurement.  Beside introducing a race condition with the new measurement command message, this mixing inescapably confuses the receiving end which expects certain sizes message and at best skews results.

Thus, to coordinate a sequence of measurements, in general, there must be a means to fully complete one before staring the next.  This can be done at the level of the shell by waiting for the ~zperfcli~ of both ends to exit before starting new ones.  Another mechanism is using the Zperf client and server.

The Zperf server accepts requests from the client for creation of one or more ~perf~ actors and will go on to accept and forward commands to bind or connect the ~perf~ measurement socket or initiate measurements and return the results.  The client side can then assure coordination.

This is an overblown solution that ~bash~ and ~ssh~ can also solve, but it gives a reason to learn how to use ~zproto~.


